{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a728ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "import pandapower as pp\n",
    "\n",
    "import random\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import pandapower.topology as top\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db3263",
   "metadata": {},
   "source": [
    "#### Inspect File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8b940b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dss/dssfs04/lwp-dss-0002/pn98cu/pn98cu-dss-0000/EliasH/PostPowerflow/113_N3253500E4301500_80638_1_32_PV100_HP100_EV100_VarTar0_CapPr0.h5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_files(path):    \n",
    "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    if not files:\n",
    "        raise FileNotFoundError(\"No files in directory.\")\n",
    "    return [os.path.join(path,file) for file in files]\n",
    "\n",
    "random.seed(41)\n",
    "files = find_files(config.input_dir)\n",
    "file_path = random.choice(files)\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ac68bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group: pwrflw\n",
      "Group: pwrflw/input\n",
      "Group: pwrflw/input/demand_post\n",
      "Group: pwrflw/input/demand_pre\n",
      "Group: pwrflw/output\n",
      "Group: pwrflw/output/post\n",
      "Group: pwrflw/output/post/demand_import\n",
      "Group: pwrflw/output/post/line_loads\n",
      "Group: pwrflw/output/post/vm\n",
      "Group: pwrflw/output/pre\n",
      "Group: pwrflw/output/pre/demand_import\n",
      "Group: pwrflw/output/pre/line_loads\n",
      "Group: pwrflw/output/pre/vm\n",
      "Group: pwrflw/urbs_out\n",
      "Group: pwrflw/urbs_out/MILP\n",
      "Group: pwrflw/urbs_out/MILP/reactive\n",
      "Group: raw_data\n",
      "Group: raw_data/buildings\n",
      "Group: raw_data/consumers\n",
      "Group: raw_data/region\n",
      "Group: raw_data/weather\n",
      "Group: urbs_in\n",
      "Group: urbs_in/buy_sell_price\n",
      "Group: urbs_in/commodity\n",
      "Group: urbs_in/demand\n",
      "Group: urbs_in/eff_factor\n",
      "Group: urbs_in/process\n",
      "Group: urbs_in/process_commodity\n",
      "Group: urbs_in/storage\n",
      "Group: urbs_in/supim\n",
      "Group: urbs_in/weather\n",
      "Group: urbs_out\n",
      "Group: urbs_out/MILP\n",
      "Group: urbs_out/MILP/cap_pro\n",
      "Group: urbs_out/MILP/cap_pro_new\n",
      "Group: urbs_out/MILP/cap_sto_c\n",
      "Group: urbs_out/MILP/cap_sto_c_new\n",
      "Group: urbs_out/MILP/cap_sto_p\n",
      "Group: urbs_out/MILP/cap_sto_p_new\n",
      "Group: urbs_out/MILP/costs\n",
      "Group: urbs_out/MILP/dt\n",
      "Group: urbs_out/MILP/e_co_buy\n",
      "Group: urbs_out/MILP/e_co_sell\n",
      "Group: urbs_out/MILP/e_pro_in\n",
      "Group: urbs_out/MILP/e_pro_out\n",
      "Group: urbs_out/MILP/e_sto_con\n",
      "Group: urbs_out/MILP/e_sto_in\n",
      "Group: urbs_out/MILP/e_sto_out\n",
      "Group: urbs_out/MILP/pro_cap_expands\n",
      "Group: urbs_out/MILP/tau_pro\n",
      "Group: urbs_out/MILP/weight\n",
      "Group: urbs_out/reduced_data\n",
      "Group: urbs_out/reduced_data/buy_sell_price\n",
      "Group: urbs_out/reduced_data/commodity\n",
      "Group: urbs_out/reduced_data/demand\n",
      "Group: urbs_out/reduced_data/eff_factor\n",
      "Group: urbs_out/reduced_data/global_prop\n",
      "Group: urbs_out/reduced_data/process\n",
      "Group: urbs_out/reduced_data/process_commodity\n",
      "Group: urbs_out/reduced_data/site\n",
      "Group: urbs_out/reduced_data/storage\n",
      "Group: urbs_out/reduced_data/supim\n",
      "Group: urbs_out/reduced_data/type_period\n",
      "Group: urbs_out/reduced_data/weather\n",
      "Group: urbs_out/tsam\n",
      "Group: urbs_out/tsam/kept_timesteps\n"
     ]
    }
   ],
   "source": [
    "def list_h5_structure(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        def visitor_func(name, obj):\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                print(f\"Group: {name}\")\n",
    "            # elif isinstance(obj, h5py.Dataset):\n",
    "            #     print(f\"Dataset: {name}\")\n",
    "        f.visititems(visitor_func)\n",
    "                \n",
    "    # with h5py.File(file_path, 'r') as f:\n",
    "    #     f.visititems(visitor_func)\n",
    "\n",
    "list_h5_structure(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91cbf3",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2daa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PV_peak_capacity_up_to(buildings, factor=0.59161):\n",
    "    total_PV_cap = sum(r[0] for roof_group in buildings[\"roofs\"] for r in roof_group)\n",
    "    return total_PV_cap*factor\n",
    "\n",
    "def create_PV_prod_up_to(supim, processes, fraction_of_total=0.6766):\n",
    "        ### Maximum installable:\n",
    "    pv_100_peak = processes[processes[\"Process\"].str.startswith(\"Rooftop PV\")].set_index([\"Site\", \"Process\"])\n",
    "    pv_100_peak = pv_100_peak[\"cap-up\"].to_frame().T\n",
    "    cap_map = {}\n",
    "    cap_cols = pv_100_peak.columns\n",
    "    for site, cap_proc in cap_cols:\n",
    "        # convert 'Rooftop PV_x_y' -> 'solar_x_y'\n",
    "        solar_proc = cap_proc.replace('Rooftop PV', 'solar')\n",
    "        cap_map[(site, solar_proc)] = pv_100_peak[(site, cap_proc)].iloc[0]\n",
    "\n",
    "    # Step 2: Build a Series indexed by df_ts.columns with the matching cap-up values\n",
    "    cap_series = pd.Series(\n",
    "        [cap_map.get((site, proc), pd.NA) for site, proc in supim.columns],\n",
    "        index=supim.columns\n",
    "    )\n",
    "\n",
    "    # Step 3: Multiply each timeseries column by its cap-up value\n",
    "    df_scaled = supim * cap_series\n",
    "\n",
    "    max_energy = df_scaled.sum().sum()\n",
    "    target_energy = max_energy * fraction_of_total\n",
    "\n",
    "    \n",
    "    supim_sums = supim.sum(axis=0)       # total capacity factor per column\n",
    "    energy_sums = df_scaled.sum(axis=0)  # total energy per column\n",
    "\n",
    "    # 2. Sort by capacity factor sum\n",
    "    order = supim_sums.sort_values(ascending=False).index\n",
    "\n",
    "    # 3. Greedy accumulation\n",
    "    selected = []\n",
    "    selected_ts = []\n",
    "    total_energy = 0.0\n",
    "\n",
    "    for col in order:\n",
    "        col_energy = energy_sums[col]\n",
    "        if total_energy + col_energy <= target_energy:\n",
    "            # take whole column\n",
    "            selected.append((col, 1.0))\n",
    "            selected_ts.append(df_scaled[col])\n",
    "            total_energy += col_energy\n",
    "        else:\n",
    "            # take only a fraction of this column\n",
    "            remaining = target_energy - total_energy\n",
    "            frac = remaining / col_energy\n",
    "            selected.append((col, frac))\n",
    "            selected_ts.append(df_scaled[col] * frac)\n",
    "            total_energy = target_energy\n",
    "            break\n",
    "\n",
    "    production_ts = pd.concat(selected_ts, axis=1).sum(axis=1)\n",
    "\n",
    "    ### Actually installed:\n",
    "    return production_ts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef56bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PV_energy_capacity(roofs, supim):\n",
    "    supim = pd.DataFrame(supim.sum(axis=0))\n",
    "    supim.rename(columns={0:\"cap\"}, inplace=True)\n",
    "\n",
    "    roofs = roofs.explode(\"roofs\")\n",
    "    roofs[\"pro\"] = roofs[\"roofs\"].apply(lambda x: f\"solar_{x[1]}_{x[2]}\")\n",
    "    roofs[\"cap\"] = roofs[\"roofs\"].apply(lambda x: x[0])\n",
    "    roofs.drop(columns=[\"roofs\"], inplace=True)\n",
    "    roofs.rename(columns={\"bus\":\"sit\"}, inplace=True)\n",
    "    roofs.set_index([\"sit\", \"pro\"], inplace=True)\n",
    "\n",
    "    return (roofs*supim).sum().values[0]\n",
    "\n",
    "def get_mobility_capacity(df_storage):\n",
    "    return df_storage[df_storage[\"Storage\"].str.startswith(\"mobility_storage\")][\"inst-cap-c\"].sum() \n",
    "\n",
    "def get_number_lines(grid):\n",
    "    trafo_buses = grid.trafo[[\"hv_bus\", \"lv_bus\"]].values[0]\n",
    "    ext_grid_bus = int(grid.ext_grid.loc[0, \"bus\"]) # bus which is the external import bus\n",
    "    lv_bus = [bus for bus in trafo_buses if bus!=ext_grid_bus][0]\n",
    "\n",
    "    # Retrieve first section of household cable strands\n",
    "    hh_strands = grid.line[grid.line[\"from_bus\"]==lv_bus].index.values     # retrieve all household strands\n",
    "    return int(len(hh_strands))\n",
    "\n",
    "def get_total_length_grid(grid):\n",
    "    return grid.line[\"length_km\"].sum()\n",
    "\n",
    "def get_total_resistance_grid(grid):\n",
    "    return (grid.line[\"length_km\"]*grid.line[\"r_ohm_per_km\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5271716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_PV_100(supim, processes):\n",
    "    cols = supim.columns\n",
    "\n",
    "    # Identify solar processes in supim (process in level 1)\n",
    "    pro_vals = cols.get_level_values(1).astype(str)\n",
    "    solar_mask = pro_vals.str.startswith(\"solar_\")\n",
    "    solar_cols = cols[solar_mask]\n",
    "\n",
    "    # Initialize per-column factor with 1.0 so non-solar stay unchanged\n",
    "    cap_factor = pd.Series(1.0, index=cols, dtype=float)\n",
    "    for sit, pro in solar_cols:\n",
    "        suffix = pro.split(\"solar_\", 1)[1]\n",
    "        mapped_proc = f\"Rooftop PV_{suffix}\"\n",
    "        cap = processes[(processes[\"Site\"] == sit) & (processes[\"Process\"] == str(mapped_proc))].loc[:, \"cap-up\"].values[0]\n",
    "        cap_factor[(sit, pro)] = float(cap)\n",
    "\n",
    "    # Multiply each column's time series by its cap factor\n",
    "    supim_cap = supim.multiply(cap_factor, axis=1)\n",
    "    solar_100 = supim_cap.sum(axis=1)\n",
    "    return solar_100\n",
    "\n",
    "\n",
    "def get_heat_and_cop(eff_factor, demand):\n",
    "    # Extract space and water heating demands (sum over all sites/processes per timestep)\n",
    "    mask = demand.columns.get_level_values(1).str.startswith(\"space_heat\")\n",
    "    space_heat = demand.loc[:, mask].droplevel(1, axis=1)\n",
    "    space_heat_tot = space_heat.sum(axis=1)\n",
    "\n",
    "    mask = demand.columns.get_level_values(1).str.startswith(\"water_heat\")\n",
    "    water_heat = demand.loc[:, mask].droplevel(1, axis=1)\n",
    "    water_heat_tot = water_heat.sum(axis=1)\n",
    "\n",
    "    # Extract heat pump COPs\n",
    "    mask = eff_factor.columns.get_level_values(1).str.startswith(\"heatpump_air\")\n",
    "    hp_cop = eff_factor.loc[:, mask].droplevel(1, axis=1)\n",
    "\n",
    "    # Weighted average COP using heat demand as weights\n",
    "    total_heat_cols = (water_heat + space_heat)  # per-column weights\n",
    "    denom = water_heat_tot + space_heat_tot      # per-timestep total weight\n",
    "    weighted_sum = hp_cop.multiply(total_heat_cols).sum(axis=1)\n",
    "\n",
    "    # Compute weighted average where denom > 0\n",
    "    weighted_cop = pd.Series(np.nan, index=denom.index, dtype=float)\n",
    "    nonzero = denom != 0\n",
    "    weighted_cop.loc[nonzero] = (weighted_sum.loc[nonzero] / denom.loc[nonzero])\n",
    "\n",
    "    # Fallback: for timesteps with zero total heat demand, use simple mean COP across columns\n",
    "    row_mean_cop = hp_cop.mean(axis=1)\n",
    "    weighted_cop.loc[~nonzero] = row_mean_cop.loc[~nonzero]\n",
    "\n",
    "    return space_heat_tot, water_heat_tot, weighted_cop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a0e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feeders(net, trafo_idx=0):\n",
    "    \"\"\"\n",
    "    For each feeder emanating from the LV-side of transformer trafo_idx in net,\n",
    "    compute total length (km), total R (Ω), number of consumer buses,\n",
    "    list of feeder-internal line segments, and list of trunk line segments\n",
    "    (from LV bus to first bus of each feeder).\n",
    "\n",
    "    Returns a dict keyed by feeder number.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Identify the LV bus of the transformer\n",
    "    lv_bus = net.trafo.lv_bus.at[trafo_idx]\n",
    "\n",
    "    # 2. Build an undirected graph of the network (buses + lines)\n",
    "    G = top.create_nxgraph(net, include_trafos=False, respect_switches=False)\n",
    "\n",
    "    # 3. Remove the trafo LV bus to split the feeders\n",
    "    G_without_trafo = G.copy()\n",
    "    G_without_trafo.remove_node(lv_bus)\n",
    "\n",
    "    # 4. Find connected components (each is one feeder)\n",
    "    components = list(nx.connected_components(G_without_trafo))\n",
    "\n",
    "    feeders = {}\n",
    "    for i, comp in enumerate(components, start=1):\n",
    "        # 4a. Feeder-internal lines: both ends in this component\n",
    "        mask_internal = net.line.apply(\n",
    "            lambda row: (row.from_bus in comp) and (row.to_bus in comp),\n",
    "            axis=1\n",
    "        )\n",
    "        feeder_segments = net.line.index[mask_internal].tolist()\n",
    "\n",
    "        # 4b. Trunk segments: those connecting LV bus to any node in comp\n",
    "        mask_trunk = net.line.apply(\n",
    "            lambda row: ((row.from_bus == lv_bus and row.to_bus in comp) or\n",
    "                         (row.to_bus == lv_bus and row.from_bus in comp)),\n",
    "            axis=1\n",
    "        )\n",
    "        trunk_segments = net.line.index[mask_trunk].tolist()\n",
    "\n",
    "        # 4c. Combine for totals if needed\n",
    "        all_segments = feeder_segments + trunk_segments\n",
    "\n",
    "        total_length = net.line.loc[all_segments, \"length_km\"].sum()\n",
    "        total_R = (net.line.loc[all_segments, \"length_km\"] *\n",
    "                   net.line.loc[all_segments, \"r_ohm_per_km\"]).sum()\n",
    "\n",
    "        # 4d. Consumer buses: buses in comp with at least one load\n",
    "        load_buses = set(net.load.bus.loc[net.load.bus.isin(comp)])\n",
    "        num_consumer_buses = len(load_buses)\n",
    "\n",
    "        feeders[f\"Feeder_{i}\"] = {\n",
    "            \"buses_in_feeder\": sorted(comp),\n",
    "            \"trunk_segments\": trunk_segments,\n",
    "            \"feeder_segments\": feeder_segments,\n",
    "            \"total_length_km\": total_length,\n",
    "            \"total_R_ohm\": total_R,\n",
    "            \"num_consumer_buses\": num_consumer_buses\n",
    "        }\n",
    "\n",
    "    return feeders\n",
    "\n",
    "def get_feeder_statistics(net, mean_line_powers):\n",
    "    list_P_R = []\n",
    "    list_nbldg_R = []\n",
    "    list_R = []\n",
    "    list_L = []\n",
    "\n",
    "    for i, feeder in analyze_feeders(net).items():\n",
    "        if feeder[\"trunk_segments\"] != []:\n",
    "            P = mean_line_powers[feeder[\"trunk_segments\"]]\n",
    "            R = feeder[\"total_R_ohm\"]\n",
    "            L = feeder[\"total_length_km\"]\n",
    "            n_bldng = feeder[\"num_consumer_buses\"]\n",
    "\n",
    "            list_P_R.append(P*R)\n",
    "            list_nbldg_R.append(n_bldng*R)\n",
    "            list_R.append(R)\n",
    "            list_L.append(L)\n",
    "\n",
    "    feeder_stats = {\n",
    "        \"max_line_nbldg_x_R\": np.max(list_nbldg_R),\n",
    "        \"max_line_P_x_R\": np.max(list_P_R),\n",
    "        \"max_line_R\": np.max(list_R),\n",
    "        \"max_line_L\": np.max(list_L)\n",
    "    }\n",
    "\n",
    "    return feeder_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e5c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_grid(grid):\n",
    "#     \"\"\" Creates an image of the network graph\n",
    "#         Args: net.line dataframe from pandapower network\n",
    "#     \"\"\"\n",
    "#     df_lines = grid.line\n",
    "#     # Create an undirected graph\n",
    "#     G = nx.Graph()\n",
    "\n",
    "#     # Add edges to the graph\n",
    "#     for _, row in df_lines.iterrows():\n",
    "#         G.add_edge(row[\"from_bus\"], row[\"to_bus\"])\n",
    "\n",
    "#     pos = nx.spring_layout(G, k=0.05, iterations=120)\n",
    "#     # Draw the graph\n",
    "#     plt.figure(figsize=(5, 4))\n",
    "#     nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=25, font_size=6, font_weight='bold', edge_color='gray')\n",
    "#     plt.title(\"Network Graph of Nodes\")\n",
    "#     plt.show()\n",
    "\n",
    "# draw_grid(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6bbd0",
   "metadata": {},
   "source": [
    "#### 1. Readout all data and concatenate from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54b6bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare storage arrays with correct dimensions\n",
    "# X timeseries input\n",
    "batch_size = len(files)\n",
    "X_ts_names = [\"demand_net_active_pre\",\"demand_net_reactive_pre\", \n",
    "              \"T\", \"GHI\", \"DNI\", \"DHI\", \"PV_prod_100\", \"PV_prod_expected\", \n",
    "              \"heat_water\", \"heat_space\", \"cop_avg\", \"mob_avail\", \"mob_last_avail\", \"mobility\",\n",
    "              \"PV_prod_true\", \"heat_true\", \"mobility_true\"  # these are the actual urbs results, should never be used apart from attribution of losses\n",
    "              ]\n",
    "X_ts_name2index = {name: i for i,name in enumerate(X_ts_names)}\n",
    "X_n_ts = len(X_ts_names)\n",
    "hours = 8760\n",
    "\n",
    "coords = {\n",
    "    \"batch\": np.arange(batch_size),\n",
    "    \"timeseries\": X_ts_names,\n",
    "    \"hour\": np.arange(hours)}\n",
    "\n",
    "ds_X_ts = xr.Dataset(   \n",
    "    {\"ts_val\": ((\"batch\", \"timeseries\", \"hour\"), np.zeros((batch_size, X_n_ts,hours), dtype=np.float32))},\n",
    "    coords=coords)\n",
    "\n",
    "\n",
    "# y timeseries target output\n",
    "y_ts_names = [\"demand_net_active_post\", \"demand_net_reactive_post\", \"demand_net_active_post_nopwrflw\", \"demand_net_reactive_post_nopwrflw\"]\n",
    "y_ts_name2index = {name: i for i,name in enumerate(y_ts_names)}\n",
    "y_n_ts = len(y_ts_names)\n",
    "\n",
    "coords = {\n",
    "    \"batch\": np.arange(batch_size),\n",
    "    \"timeseries\": y_ts_names,\n",
    "    \"hour\": np.arange(hours)}\n",
    "\n",
    "ds_y_ts = xr.Dataset(   \n",
    "    {\"ts_val\": ((\"batch\", \"timeseries\", \"hour\"), np.zeros((batch_size, y_n_ts,hours), dtype=np.float32))},\n",
    "    coords=coords)\n",
    "\n",
    "# X scalar input\n",
    "X_sclr_names = [\"n_bldng\",\"n_flats\",\"n_occ\",\"bldng_area_base_sum\",\"res_bldng_area_base_sum\", \"nonres_bldng_area_base_sum\",\"bldng_area_floors_sum\",\n",
    "                \"n_radiator\", \"n_res_buildings\", \"n_nonres_buildings\",\n",
    "                \"PV_cap_power_max_sum\",\"PV_cap_power_exp_sum\",\"PV_cap_energy_max_sum\",\n",
    "                \"regiostar7\", \"n_cars\", \"car_battery_cap\",\n",
    "                \"n_lines\",\"tot_len_grid\",\"tot_R_grid\",\n",
    "                \"max_line_nbldg_x_R\",\"max_line_P_x_R\",\"max_line_R\",\"max_line_L\"]\n",
    "X_sclr_name2index = {name: i for i,name in enumerate(X_sclr_names)}\n",
    "X_n_sclr = len(X_sclr_names)\n",
    "\n",
    "coords = coords = {\n",
    "    \"batch\": np.arange(batch_size),\n",
    "    \"scalar\": X_sclr_names}\n",
    "\n",
    "ds_X_sclr = xr.Dataset(   \n",
    "    {\"sclr_val\": ((\"batch\", \"scalar\"), np.zeros((batch_size,X_n_sclr), dtype=np.float32))},\n",
    "    coords=coords)\n",
    "\n",
    "\n",
    "# y scalar target output\n",
    "y_sclr_names = [\"demand_net_active_agg_post\",\"consumption_net_active_agg_post\",\"feedin_net_active_agg_post\",\n",
    "                \"demand_net_reactive_agg_post\",\"consumption_net_active_peak_post\",\"feedin_net_active_peak_post\",\n",
    "                \"voltage_violation_lower_min\",\"voltage_violation_upper_max\"]\n",
    "y_sclr_name2index = {name: i for i,name in enumerate(y_sclr_names)}\n",
    "y_n_sclr = len(y_sclr_names)\n",
    "\n",
    "coords = coords = {\n",
    "    \"batch\": np.arange(batch_size),\n",
    "    \"scalar\": y_sclr_names}\n",
    "\n",
    "ds_y_sclr = xr.Dataset(   \n",
    "    {\"sclr_val\": ((\"batch\", \"scalar\"), np.zeros((batch_size,y_n_sclr), dtype=np.float32))},\n",
    "    coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e059856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far processed 100 files...\n",
      "So far processed 200 files...\n",
      "So far processed 300 files...\n",
      "So far processed 400 files...\n",
      "So far processed 500 files...\n",
      "So far processed 600 files...\n",
      "So far processed 700 files...\n",
      "So far processed 800 files...\n",
      "So far processed 900 files...\n",
      "So far processed 1000 files...\n",
      "So far processed 1100 files...\n"
     ]
    }
   ],
   "source": [
    "### Loop over all files and read-out wanted inputs\n",
    "for i, file in enumerate(files):\n",
    "    if i % 100 == 0 and i>0: print(f\"So far processed {i} files...\")\n",
    "    with pd.HDFStore(file) as store:\n",
    "        # X timeseries input:\n",
    "        j = X_ts_name2index[\"demand_net_active_pre\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/output/pre/demand_import\"][\"p_mw\"].values\n",
    "\n",
    "        j = X_ts_name2index[\"demand_net_reactive_pre\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/output/pre/demand_import\"][\"q_mvar\"].values\n",
    "\n",
    "        j = X_ts_name2index[\"T\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"raw_data/weather\"][\"temp_air\"].values\n",
    "\n",
    "        j = X_ts_name2index[\"GHI\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"raw_data/weather\"][\"ghi\"].values\n",
    "\n",
    "        j = X_ts_name2index[\"DNI\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"raw_data/weather\"][\"dni\"].values\n",
    "\n",
    "        j = X_ts_name2index[\"DHI\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = store[\"raw_data/weather\"][\"dhi\"].values\n",
    "\n",
    "        supim = store[\"urbs_in/supim\"]\n",
    "        processes = store[\"urbs_in/process\"]\n",
    "        demand = store[\"urbs_in/demand\"]\n",
    "        eff_factor = store[\"urbs_in/eff_factor\"]\n",
    "        e_pro_out = store[\"urbs_out/MILP/e_pro_out\"]\n",
    "        e_pro_in = store[\"urbs_out/MILP/e_pro_in\"]\n",
    "\n",
    "        mask = e_pro_out.index.get_level_values(\"pro\").str.startswith(\"Rooftop PV\")\n",
    "        PV_prod_true = e_pro_out[mask]\n",
    "        PV_prod_true = PV_prod_true.reset_index()\n",
    "        PV_prod_true = PV_prod_true.pivot_table(\n",
    "            index=\"t\",\n",
    "            columns=[\"sit\", \"pro\"],\n",
    "            values=PV_prod_true.columns.difference([\"t\", \"stf\", \"sit\", \"pro\", \"com\"])[0])\n",
    "        j = X_ts_name2index[\"PV_prod_true\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = PV_prod_true.sum(axis=1).values\n",
    "\n",
    "        j = X_ts_name2index[\"PV_prod_expected\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = create_PV_prod_up_to(supim.copy(), processes.copy()).values\n",
    "\n",
    "        j = X_ts_name2index[\"PV_prod_100\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = get_PV_100(supim.copy(), processes.copy()).values\n",
    "\n",
    "        heat_space, heat_water, cop_avg = get_heat_and_cop(eff_factor.copy(), demand.copy())\n",
    "        j = X_ts_name2index[\"heat_space\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = heat_space.values\n",
    "        j = X_ts_name2index[\"heat_water\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = heat_water.values\n",
    "        j = X_ts_name2index[\"cop_avg\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = cop_avg.values\n",
    "\n",
    "        mask = e_pro_in.index.get_level_values(\"pro\").str.startswith(\"heatpump_air\")\n",
    "        heat_dem_HP_true = e_pro_in[mask]\n",
    "        heat_dem_HP_true = heat_dem_HP_true.reset_index()\n",
    "        heat_dem_HP_true = heat_dem_HP_true.pivot_table(\n",
    "            index=\"t\",\n",
    "            columns=[\"sit\", \"pro\"],\n",
    "            values=heat_dem_HP_true.columns.difference([\"t\", \"stf\", \"sit\", \"pro\", \"com\"])[0])\n",
    "        heat_dem_HP_true = heat_dem_HP_true.sum(axis=1)\n",
    "        mask = e_pro_in.index.get_level_values(\"pro\").str.startswith(\"heatpump_booster\")\n",
    "        heat_dem_bst_true = e_pro_in[mask]\n",
    "        heat_dem_bst_true = heat_dem_bst_true.reset_index()\n",
    "        heat_dem_bst_true = heat_dem_bst_true.pivot_table(\n",
    "            index=\"t\",\n",
    "            columns=[\"sit\", \"pro\"],\n",
    "            values=heat_dem_bst_true.columns.difference([\"t\", \"stf\", \"sit\", \"pro\", \"com\"])[0])\n",
    "        heat_dem_bst_true = heat_dem_bst_true.sum(axis=1)\n",
    "        j = X_ts_name2index[\"heat_true\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = (heat_dem_HP_true.add(heat_dem_bst_true, fill_value=0)).values\n",
    "\n",
    "        mask = eff_factor.columns.get_level_values(1).str.startswith(\"charging_station\")\n",
    "        mobility = eff_factor.loc[:, mask]\n",
    "        mobility_avail = mobility.sum(axis=1)\n",
    "        j = X_ts_name2index[\"mob_avail\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = mobility_avail.values\n",
    "\n",
    "        mask_last = (mobility == 1) & (mobility.shift(-1) == 0)\n",
    "        mobility_last_avail = mobility.where(mask_last).sum(axis=1)\n",
    "        j = X_ts_name2index[\"mob_last_avail\"]\n",
    "        ds_X_ts[\"ts_val\"].data[i,j,:] = mobility_last_avail.values\n",
    "\n",
    "        try:\n",
    "            mask = demand.columns.get_level_values(1).str.startswith(\"mobility\")\n",
    "            j = X_ts_name2index[\"mobility\"]\n",
    "            ds_X_ts[\"ts_val\"].data[i,j,:] = demand.loc[:,mask].sum(axis=1).values\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            mask = e_pro_in.index.get_level_values(\"pro\").str.startswith(\"charging_station\")\n",
    "            mob_dem_true = e_pro_in[mask]\n",
    "            mob_dem_true = mob_dem_true.reset_index()\n",
    "            mob_dem_true = mob_dem_true.pivot_table(\n",
    "                index=\"t\",\n",
    "                columns=[\"sit\", \"pro\"],\n",
    "                values=mob_dem_true.columns.difference([\"t\", \"stf\", \"sit\", \"pro\", \"com\"])[0])\n",
    "            j = X_ts_name2index[\"mobility_true\"]\n",
    "            ds_X_ts[\"ts_val\"].data[i,j,:] = mob_dem_true.sum(axis=1).values\n",
    "        except (ValueError, KeyError):\n",
    "            pass\n",
    "\n",
    "        # y timeseries target\n",
    "        j = y_ts_name2index[\"demand_net_active_post\"]\n",
    "        ds_y_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].values\n",
    "\n",
    "        j = y_ts_name2index[\"demand_net_reactive_post\"]\n",
    "        ds_y_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/output/post/demand_import\"][\"q_mvar\"].values\n",
    "\n",
    "        j = y_ts_name2index[\"demand_net_active_post_nopwrflw\"]\n",
    "        ds_y_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/input/demand_post\"].xs('electricity', axis=1, level=1, drop_level=False).sum(axis=1)\n",
    "\n",
    "        j = y_ts_name2index[\"demand_net_reactive_post_nopwrflw\"]\n",
    "        ds_y_ts[\"ts_val\"].data[i,j,:] = store[\"pwrflw/input/demand_post\"].xs('electricity-reactive', axis=1, level=1, drop_level=False).sum(axis=1)\n",
    "\n",
    "\n",
    "        # X scalar input\n",
    "        j = X_sclr_name2index[\"n_bldng\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = len(store[\"raw_data/buildings\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"n_flats\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = int(store[\"raw_data/buildings\"][\"houses_per_building\"].sum())\n",
    "\n",
    "        j = X_sclr_name2index[\"n_occ\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = int(round(sum(store[\"raw_data/buildings\"][\"occ_list\"].sum()),0))\n",
    "\n",
    "        j = X_sclr_name2index[\"bldng_area_base_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = store[\"raw_data/buildings\"][\"area\"].sum()\n",
    "\n",
    "        j = X_sclr_name2index[\"bldng_area_floors_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = (store[\"raw_data/buildings\"][\"area\"]*store[\"raw_data/buildings\"][\"floors\"]).sum()\n",
    "\n",
    "        buildings = store[\"raw_data/buildings\"]\n",
    "        j = X_sclr_name2index[\"n_res_buildings\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = len(buildings[buildings[\"use\"]==\"Residential\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"n_nonres_buildings\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = len(buildings[buildings[\"use\"]!=\"Residential\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"res_bldng_area_base_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = buildings[buildings[\"use\"]==\"Residential\"][\"area\"].sum()\n",
    "\n",
    "        j = X_sclr_name2index[\"nonres_bldng_area_base_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = buildings[buildings[\"use\"]!=\"Residential\"][\"area\"].sum()\n",
    "        \n",
    "        j = X_sclr_name2index[\"n_radiator\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = len(buildings[buildings[\"heating_type\"]==\"radiator\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"PV_cap_power_max_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = sum([roofs[0] for buildings in store[\"raw_data/buildings\"][\"roofs\"].values for roofs in buildings])\n",
    "\n",
    "        j = X_sclr_name2index[\"PV_cap_power_exp_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_PV_peak_capacity_up_to(store[\"raw_data/buildings\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"PV_cap_energy_max_sum\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_PV_energy_capacity(store[\"raw_data/buildings\"][[\"bus\",\"roofs\"]], store[\"urbs_in/supim\"])\n",
    "\n",
    "        j = X_sclr_name2index[\"regiostar7\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = store[\"raw_data/region\"][\"regio7\"]\n",
    "\n",
    "        j = X_sclr_name2index[\"n_cars\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = int(store[\"raw_data/buildings\"][\"n_cars_tot\"].sum())\n",
    "\n",
    "        j = X_sclr_name2index[\"car_battery_cap\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_mobility_capacity(store[\"urbs_in/storage\"])\n",
    "\n",
    "        # y scalar target\n",
    "        [\"consumption_net_active_peak_post\",\"feedin_net_active_peak_post\",\"voltage_violation_lower_min\",\"voltage_violation_upper_max\"]\n",
    "        j = y_sclr_name2index[\"demand_net_active_agg_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].sum()\n",
    "\n",
    "        j = y_sclr_name2index[\"consumption_net_active_agg_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].clip(lower=0).sum()\n",
    "\n",
    "        j = y_sclr_name2index[\"feedin_net_active_agg_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].clip(upper=0).sum()\n",
    "\n",
    "        j = y_sclr_name2index[\"demand_net_reactive_agg_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"q_mvar\"].sum()\n",
    "\n",
    "        j = y_sclr_name2index[\"consumption_net_active_peak_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].max()\n",
    "\n",
    "        j = y_sclr_name2index[\"feedin_net_active_peak_post\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/demand_import\"][\"p_mw\"].min()\n",
    "\n",
    "        j = y_sclr_name2index[\"voltage_violation_lower_min\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/vm\"].min().min()\n",
    "\n",
    "        j = y_sclr_name2index[\"voltage_violation_upper_max\"]\n",
    "        ds_y_sclr[\"sclr_val\"].data[i,j] = store[\"pwrflw/output/post/vm\"].max().max()\n",
    "\n",
    "        # Extract line loads for following analysis\n",
    "        line_loads = store[\"pwrflw/output/pre/line_loads\"]\n",
    "        line_p_flow = line_loads.loc[:, line_loads.columns.get_level_values(1) == 'p_from_mw'].mean(axis=0)\n",
    "\n",
    "    # Extra X scalar input data which needs different readout\n",
    "    with h5py.File(file, 'r') as file:\n",
    "        net = pp.from_json_string(file[\"raw_data/net\"][()])\n",
    "        line_stats = get_feeder_statistics(net, line_p_flow)\n",
    "\n",
    "        j = X_sclr_name2index[\"n_lines\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_number_lines(net)\n",
    "\n",
    "        j = X_sclr_name2index[\"tot_len_grid\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_total_length_grid(net)\n",
    "\n",
    "        j = X_sclr_name2index[\"tot_R_grid\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = get_total_resistance_grid(net)\n",
    "\n",
    "        j = X_sclr_name2index[\"max_line_nbldg_x_R\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = line_stats[\"max_line_nbldg_x_R\"]\n",
    "\n",
    "        j = X_sclr_name2index[\"max_line_P_x_R\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = line_stats[\"max_line_P_x_R\"]\n",
    "\n",
    "        j = X_sclr_name2index[\"max_line_R\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = line_stats[\"max_line_R\"]\n",
    "\n",
    "        j = X_sclr_name2index[\"max_line_L\"]\n",
    "        ds_X_sclr[\"sclr_val\"].data[i,j] = line_stats[\"max_line_L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "FILEPATH = \"Data/all_data.h5\"\n",
    "\n",
    "ds_X_ts.to_netcdf(\n",
    "    path=FILEPATH,\n",
    "    mode=\"w\",               # create or overwrite\n",
    "    format=\"NETCDF4\",       # HDF5-based netCDF4 format\n",
    "    engine=\"h5netcdf\",      # h5netcdf backend\n",
    "    group=\"X_ts\"            # subgroup name (no leading slash)\n",
    ")\n",
    "\n",
    "ds_y_ts.to_netcdf(\n",
    "    path=FILEPATH,\n",
    "    mode=\"a\",               # append, do NOT overwrite\n",
    "    format=\"NETCDF4\",       # same format\n",
    "    engine=\"h5netcdf\",      # same engine\n",
    "    group=\"y_ts\"\n",
    ")\n",
    "\n",
    "ds_X_sclr.to_netcdf(\n",
    "    path=FILEPATH,\n",
    "    mode=\"a\",\n",
    "    format=\"NETCDF4\",\n",
    "    engine=\"h5netcdf\",\n",
    "    group=\"X_sclr\"\n",
    ")\n",
    "\n",
    "ds_y_sclr.to_netcdf(\n",
    "    path=FILEPATH,\n",
    "    mode=\"a\",\n",
    "    format=\"NETCDF4\",\n",
    "    engine=\"h5netcdf\",\n",
    "    group=\"y_sclr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00875b45",
   "metadata": {},
   "source": [
    "### 2. Train-Test Split + Conversion To Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7b3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Readout all inputs\n",
    "FILEPATH = \"Data/all_data.h5\"\n",
    "# Read the dataset from group \"X_ts\"\n",
    "ds_X_ts = xr.open_dataset(\n",
    "    FILEPATH,\n",
    "    group=\"X_ts\",\n",
    "    engine=\"h5netcdf\")\n",
    "# Read the dataset from group \"y_ts\"\n",
    "ds_y_ts = xr.open_dataset(\n",
    "    FILEPATH,\n",
    "    group=\"y_ts\",\n",
    "    engine=\"h5netcdf\")\n",
    "# Read the dataset from group \"X_sclr\"\n",
    "ds_X_sclr = xr.open_dataset(\n",
    "    FILEPATH,\n",
    "    group=\"X_sclr\",\n",
    "    engine=\"h5netcdf\")\n",
    "# Read the dataset from group \"y_sclr\"\n",
    "ds_y_sclr = xr.open_dataset(\n",
    "    FILEPATH,\n",
    "    group=\"y_sclr\",\n",
    "    engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7edf92cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train-Test-Split\n",
    "seed=42\n",
    "\n",
    "n_batch = ds_X_ts.sizes['batch']\n",
    "batch_idxs = np.arange(n_batch)\n",
    "train_idx, test_idx = train_test_split(\n",
    "    batch_idxs,\n",
    "    test_size=1/6,      # 200/1200 for test\n",
    "    random_state=seed,\n",
    "    shuffle=True)\n",
    "\n",
    "ds_X_ts_train = ds_X_ts.isel(batch=train_idx)\n",
    "ds_X_ts_test  = ds_X_ts.isel(batch=test_idx)\n",
    "\n",
    "ds_y_ts_train = ds_y_ts.isel(batch=train_idx)\n",
    "ds_y_ts_test  = ds_y_ts.isel(batch=test_idx)\n",
    "\n",
    "ds_X_sclr_train = ds_X_sclr.isel(batch=train_idx)\n",
    "ds_X_sclr_test  = ds_X_sclr.isel(batch=test_idx)\n",
    "\n",
    "ds_y_sclr_train = ds_y_sclr.isel(batch=train_idx)\n",
    "ds_y_sclr_test  = ds_y_sclr.isel(batch=test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6917180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Readout all inputs\n",
    "# FILEPATH = \"../../0_preprocessing/Data/train.h5\"\n",
    "# # Read the dataset from group \"X_ts\"\n",
    "# ds_X_ts = xr.open_dataset(\n",
    "#     FILEPATH,\n",
    "#     group=\"X_ts\",\n",
    "#     engine=\"h5netcdf\")\n",
    "# # Read the dataset from group \"y_ts\"\n",
    "# ds_y_ts = xr.open_dataset(\n",
    "#     FILEPATH,\n",
    "#     group=\"y_ts\",\n",
    "#     engine=\"h5netcdf\")\n",
    "# # Read the dataset from group \"X_sclr\"\n",
    "# ds_X_sclr = xr.open_dataset(\n",
    "#     FILEPATH,\n",
    "#     group=\"X_sclr\",\n",
    "#     engine=\"h5netcdf\")\n",
    "# # Read the dataset from group \"y_sclr\"\n",
    "# ds_y_sclr = xr.open_dataset(\n",
    "#     FILEPATH,\n",
    "#     group=\"y_sclr\",\n",
    "#     engine=\"h5netcdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f56d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_timeseries_df(ds_X_ts, ds_X_sclr, ds_y_ts):\n",
    "    ### Convert X timeseries to DataFrame\n",
    "    df_long = ds_X_ts[\"ts_val\"].to_dataframe()\n",
    "    df_X_ts = df_long.reset_index().pivot_table(\n",
    "        index=['batch', 'hour'],           # the row index levels\n",
    "        columns='timeseries',            # the former series‑dimension\n",
    "        values='ts_val',                     # the cell contents\n",
    "        aggfunc='last'\n",
    "    )\n",
    "    df_X_ts.index.names = ['batch', 'hour']\n",
    "    df_X_ts.columns.name = None  # drop the name ‘n_timeseries’\n",
    "\n",
    "    ### Convert X scalar to DataFrame\n",
    "    df_long = ds_X_sclr[\"sclr_val\"].to_dataframe()\n",
    "    df_X_sclr = df_long.reset_index().pivot(\n",
    "        index=['batch'],           # the row index levels\n",
    "        columns='scalar',            # the former series‑dimension\n",
    "        values='sclr_val'                     # the cell contents\n",
    "    )\n",
    "    df_X_sclr.index.names = ['batch']\n",
    "    df_X_sclr.columns.name = None  # drop the name ‘n_timeseries’\n",
    "\n",
    "    ### Combine:\n",
    "    df_X = df_X_ts.join(df_X_sclr, on='batch')\n",
    "\n",
    "    ### Convert y timeseries to DataFrame\n",
    "    df_long = ds_y_ts[\"ts_val\"].to_dataframe()\n",
    "    df_y = df_long.reset_index().pivot(\n",
    "        index=['batch', 'hour'],           # the row index levels\n",
    "        columns='timeseries',            # the former series‑dimension\n",
    "        values='ts_val'                     # the cell contents\n",
    "    )\n",
    "    df_y.index.names = ['batch', 'hour']\n",
    "    df_y.columns.name = None  # drop the name ‘n_timeseries’\n",
    "\n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdfab15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train, df_y_train = create_timeseries_df(ds_X_ts_train, ds_X_sclr_train, ds_y_ts_train)\n",
    "df_X_train.to_hdf(\"Data/ts_train.h5\", key=\"X\", mode=\"w\")\n",
    "df_y_train.to_hdf(\"Data/ts_train.h5\", key=\"y\", mode=\"a\")\n",
    "\n",
    "df_X_test, df_y_test = create_timeseries_df(ds_X_ts_test, ds_X_sclr_test, ds_y_ts_test)\n",
    "df_X_test.to_hdf(\"Data/ts_test.h5\", key=\"X\", mode=\"w\")\n",
    "df_y_test.to_hdf(\"Data/ts_test.h5\", key=\"y\", mode=\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48cc68",
   "metadata": {},
   "source": [
    "### (3. Only for small grid approach of thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c596dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = pd.read_hdf(\"Data/ts_train.h5\", key=\"X\")\n",
    "df_train_y = pd.read_hdf(\"Data/ts_train.h5\", key=\"y\")\n",
    "df_test_X = pd.read_hdf(\"Data/ts_test.h5\", key=\"X\")\n",
    "df_test_y = pd.read_hdf(\"Data/ts_test.h5\", key=\"y\")\n",
    "\n",
    "def _filter_small_batches(df_X, df_y, thresh=4):\n",
    "    # Get per-batch building counts (scalars are constant across hours)\n",
    "    bld_counts = (\n",
    "        df_X.groupby(level=\"batch\")[[\"n_res_buildings\", \"n_nonres_buildings\"]]\n",
    "            .first()\n",
    "            .sum(axis=1)\n",
    "    )\n",
    "    keep_batches = bld_counts[bld_counts > thresh].index\n",
    "    df_X_f = df_X.loc[keep_batches]\n",
    "    df_y_f = df_y.loc[keep_batches]\n",
    "    return df_X_f, df_y_f\n",
    "\n",
    "df_train_X, df_train_y = _filter_small_batches(df_train_X, df_train_y)\n",
    "df_test_X, df_test_y   = _filter_small_batches(df_test_X, df_test_y)\n",
    "\n",
    "with pd.HDFStore(\"Data/ts_train_large_grids.h5\", mode=\"w\") as store:\n",
    "    store[\"X\"] = df_train_X\n",
    "    store[\"y\"] = df_train_y\n",
    "with pd.HDFStore(\"Data/ts_test_large_grids.h5\", mode=\"w\") as store:\n",
    "    store[\"X\"] = df_test_X\n",
    "    store[\"y\"] = df_test_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GridForecast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
